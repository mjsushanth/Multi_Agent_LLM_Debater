# Multi-Agent LLM Debate Framework

A modular framework for orchestrating structured debates between multiple large language models (LLMs) with specialized judge evaluation. This project implements an adversarial training approach to enhance LLM argumentative reasoning.

## Features

- **Multi-Agent Architecture**: Orchestrates debates between opposing LLM agents
- **Structured Debate Protocol**: Implements formal opening, rebuttal, and closing rounds
- **Adversarial Critique System**: Agents analyze and critique opposing arguments
- **Evidence Self-Check Mechanism**: Ensures factual accuracy and reduces source fabrication
- **Multi-Dimensional Judge Framework**: Seven specialized judges evaluate different aspects of argument quality
- **Local-Based**: Compatible with Ollama-hosted models

## Requirements

- Python 3.8+
- [Ollama](https://ollama.ai/) for local model hosting
- YAML for configuration files
- Required Python packages (see Environment Setup)

## Installation

### Environment Setup

1. Clone this repository:
   ```bash
   git clone https://github.com/[username]/multi-agent-llm-debate.git
   cd multi-agent-llm-debate
   ```

2. Create and activate the conda environment:
   ```bash
   conda env create -f debate-env.yml
   conda activate debate-env
   ```

3. Install Ollama following instructions at [ollama.ai](https://ollama.ai/download)

4. Download required models via Ollama: It's present in the first cell code which can be edited. Selective downloads / Download All can be done.


### High-Level Debate Orchestration Flow:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           PREPARATION & CONFIG LAYER                         â”‚
â”‚  YAML Prompts â†’ Theory Integration â†’ Judge Config â†’ Model Selection         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           DEBATE EXECUTION LAYER                             â”‚
â”‚  Agent Init â†’ Round Control â†’ Evidence Check â†’ Critique Gen â†’ Response      â”‚
â”‚  (FOR/AGAINST) â†’ (Opening/Rebuttal/Closing) â†’ (Invisible Prep) â†’ (Output)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         MULTI-JUDGE EVALUATION LAYER                         â”‚
â”‚  7 Specialized Judges â†’ Parallel Scoring â†’ Consensus Algorithm â†’ Meta-Judge â”‚
â”‚  (Logic/Fact/Rhetoric/Strategy/Ethics/Belief/Audience) â†’ Weighted Aggregate â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         STORAGE & PERSISTENCE LAYER                          â”‚
â”‚  JSON Debate Logs â†’ Judgment Records â†’ Transcript Generation â†’ Results API  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Preparation Pipeline (Per Round)

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  EVIDENCE CHECK TRACK:
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Previous Response â†’ Claim Extraction â†’ Source Verification â†’ Strength Analysis
        â†“                    â†“                   â†“                    â†“
  [Self-Critique]    [Find Claims]    [Check Citations]    [Rate: Strong/Med/Weak]
                                                                      â†“
                                                            {evidence_report.json}
                                                                      â†“
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                                                      â†“
  ADVERSARIAL CRITIQUE TRACK:                                        â†“
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â†“
  Opponent Argument â†’ 5-Dim Weakness Detection â†’ Vulnerability Map   â†“
        â†“                         â†“                      â†“           â†“
  [Latest Args]    [Logic/Fact/Assume/Rhetoric/Strategy] [Counter]   â†“
                                                            â†“         â†“
                                                    {critique.json}   â†“
                                                            â†“         â†“
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                                            â†“         â†“
  ENHANCED PROMPT ASSEMBLY:                                â†“         â†“
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                               â†“         â†“
  Base Debate Prompt + {evidence_report} + {critique} â†’ Merge â†’ Token Optimize â†’ Final Prompt
                                                                        â†“
                                                              [Generate Response]
                                                                        â†“
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Evaluation & Consensus Architecture
```
INPUT: {combined_arguments, topic, stance, word_limit}
                            â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              PARALLEL JUDGE EVALUATION                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                       â•‘
    â•‘  LOGICAL_JUDGE     â†’ Fallacy Detection                â•‘ â†’ Score: 8.1/10
    â•‘                      Internal Consistency             â•‘   Critique: 300 words
    â•‘                      Reasoning Chains                 â•‘
    â•‘                                                       â•‘
    â•‘  FACTUAL_JUDGE     â†’ Source Verification              â•‘ â†’ Score: 7.4/10
    â•‘                      Evidence Quality                 â•‘   Critique: 300 words
    â•‘                      Citation Integrity               â•‘
    â•‘                                                       â•‘
    â•‘  RHETORICAL_JUDGE  â†’ Persuasion Analysis              â•‘ â†’ Score: 8.5/10
    â•‘                      Emotional Appeal                 â•‘   Critique: 300 words
    â•‘                      Language Effectiveness           â•‘
    â•‘                                                       â•‘
    â•‘  STRATEGIC_JUDGE   â†’ Argument Selection               â•‘ â†’ Score: 7.8/10
    â•‘                      Adaptive Response                â•‘   Critique: 300 words
    â•‘                      Framing Control                  â•‘
    â•‘                                                       â•‘
    â•‘  ETHICAL_JUDGE     â†’ Fair Representation              â•‘ â†’ Score: 9.2/10
    â•‘                      Intellectual Honesty             â•‘   Critique: 300 words
    â•‘                      Respectful Conduct               â•‘
    â•‘                                                       â•‘
    â•‘  BELIEF_JUDGE      â†’ Audience Impact                  â•‘ â†’ Score: 6.9/10
    â•‘                      Mind-Change Potential            â•‘   Critique: 300 words
    â•‘                      Cross-Segment Appeal             â•‘
    â•‘                                                       â•‘
    â•‘  AUDIENCE_JUDGE    â†’ Comprehension (4 dims)           â•‘ â†’ Score: 7.5/10
    â•‘                      Engagement Metrics               â•‘   Panel Response: 300 words
    â•‘                                                       â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                            â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚               META-JUDGE CONSENSUS                    â”‚
    â”‚                                                       â”‚
    â”‚  â€¢ Inter-Judge Correlation (r = 0.64-0.91)          â”‚
    â”‚  â€¢ Composite Score Calculation                       â”‚
    â”‚                                                       â”‚
    â”‚  FINAL OUTPUT:                                       â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
    â”‚  Composite Score: 7.7/10                            â”‚
    â”‚  Consensus Strengths: [...]                         â”‚
    â”‚  Consensus Weaknesses: [...]                        â”‚
    â”‚  Definitive Assessment: 300 words                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```



## ğŸ“ Project Structure

```
.
â”œâ”€â”€ .ipynb_checkpoints/     # Jupyter notebook checkpoints
â”œâ”€â”€ prompts/                # YAML configuration files for debate prompts
â”‚   â”œâ”€â”€ debate_prompts.yml  # Core debate prompts
â”‚   â”œâ”€â”€ judge_prompts.yml   # Judge evaluation prompts
â”œâ”€â”€ results/                # Debate outputs and judge evaluations
â”‚   â”œâ”€â”€ agent_records/      # Saved debate transcripts
â”‚   â”œâ”€â”€ judge_records/      # Evaluation results
â”‚   â”œâ”€â”€ perfect_debate_transcripts/ # Curated debate examples, for Judgement Pipeline
â”œâ”€â”€ debate-env.yml          # Conda environment configuration
â”œâ”€â”€ MultiLLM Debate.ipynb   # Main notebook for running debates
â”œâ”€â”€ OLLAMA EDA, Test Scripts.ipynb # Ollama exploration and testing scripts
```

## Core Components

### 1. Prompt Management System

- `PromptManager` class loads and formats debate prompts from YAML files
- Modular design allows testing different prompt strategies
- Phase-specific guidance for opening, rebuttal, and closing rounds

### 2. Multi-Agent Debate Engine

- `MultiAgentDebate` class orchestrates structured interactions
- Implements preparation, critique, and rebuttal phases
- Manages context and maintains debate state
- Generates enhanced arguments based on adversarial feedback

### 3. Judge Evaluation Pipeline

- `JudgeEvaluator` class assesses debate quality across multiple dimensions
- Specialized judges for logical, factual, rhetorical, and ethical aspects
- Meta-judge synthesizes evaluations into composite assessment


## Customization

### Modifying Debate Prompts

Edit the YAML files in the `prompts/` directory to customize:
- Debate instructions and structure
- Critique guidelines
- Evidence check parameters
- Judge evaluation criteria


### Adding New Models

Update the `OllamaDebateManager.models` dictionary to include new models:

```python
self.models = {
    "custom_model": "model_name:tag",
    # Add more models here
}
```

## Results and Evaluation

Debate results and judge evaluations are saved to:
- `results/agent_records/` - Full debate transcripts
- `results/judge_records/` - Judge evaluations and scores


## Skills Picked Up:
- Agent Coordination, Persistent Memory Systems, Inter-Agent Communication, Scalable Agent Framework.
- API Integration.
- Model Orchestration (Custom class handling model lifecycle, health checks, and failover mechanisms.)
- Assessment: Scoring Algorithms, Meta-Evaluation, Performance Metrics.
- YAML-Based Configuration, Parameter Management.

## Citation
If you use this framework in your research, please cite:

```
@misc{markapudi2025socraiticcircle,
  title={SocrAItic Circle: Enhancing LLM Reasoning Through Multi-Agent Debate Frameworks},
  author={Markapudi, Joel},
  year={2025},
  institution={Northeastern University}
}
```

## Contributions
TBD

## License
Authorship of all Code Notebooks, Environment Setup, Prompts Files - Joel Markapudi.  
